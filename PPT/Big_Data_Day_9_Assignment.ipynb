{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **TOPIC: Docker**"
      ],
      "metadata": {
        "id": "NQy_VI9pPIT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Scenario: You are building a microservices-based application using Docker. Design a Docker Compose file that sets up three containers: a web server container, a database container, and a cache container. Ensure that the containers can communicate with each other properly."
      ],
      "metadata": {
        "id": "hk2JzWghPOiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-S-rsn4Optp"
      },
      "outputs": [],
      "source": [
        "version: '3'\n",
        "services:\n",
        "  web:\n",
        "    build:\n",
        "      context: .\n",
        "      dockerfile: Dockerfile\n",
        "    ports:\n",
        "      - 80:80\n",
        "    depends_on:\n",
        "      - database\n",
        "      - cache\n",
        "\n",
        "  database:\n",
        "    image: mysql:latest\n",
        "    environment:\n",
        "      MYSQL_ROOT_PASSWORD: example\n",
        "      MYSQL_DATABASE: myapp\n",
        "      MYSQL_USER: myuser\n",
        "      MYSQL_PASSWORD: mypassword\n",
        "\n",
        "  cache:\n",
        "    image: redis:latest\n",
        "\n",
        "\n",
        "\n",
        "#In the same directory of docker created, run the docker by executing below command\n",
        "docker-compose up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Scenario: You want to scale your Docker containers dynamically based on the incoming traffic. Write a Python script that utilizes Docker SDK to monitor the CPU usage of a container and automatically scales the number of replicas based on a threshold."
      ],
      "metadata": {
        "id": "Q0MzcZq3P4R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import docker\n",
        "import psutil\n",
        "\n",
        "# Set the threshold for CPU usage in percentage\n",
        "cpu_threshold = 80\n",
        "\n",
        "# Docker client setup\n",
        "client = docker.from_env()\n",
        "\n",
        "def get_container_cpu_usage(container):\n",
        "    \"\"\"Get the CPU usage of a container\"\"\"\n",
        "    container_stats = container.stats(stream=False)\n",
        "    cpu_stats = container_stats['cpu_stats']\n",
        "    cpu_delta = cpu_stats['cpu_usage']['total_usage'] - cpu_stats['precpu_stats']['cpu_usage']['total_usage']\n",
        "    system_delta = cpu_stats['system_cpu_usage'] - cpu_stats['precpu_stats']['system_cpu_usage']\n",
        "    cpu_percent = (cpu_delta / system_delta) * len(cpu_stats['cpu_usage']['percpu_usage']) * 100\n",
        "    return cpu_percent\n",
        "\n",
        "def scale_containers(service, replicas):\n",
        "    \"\"\"Scale the number of replicas of a service\"\"\"\n",
        "    service.scale(replicas)\n",
        "\n",
        "def monitor_containers():\n",
        "    \"\"\"Monitor the CPU usage of containers and scale if necessary\"\"\"\n",
        "    containers = client.containers.list()\n",
        "    for container in containers:\n",
        "        cpu_percent = get_container_cpu_usage(container)\n",
        "        print(f\"Container {container.name} - CPU Usage: {cpu_percent}%\")\n",
        "        if cpu_percent > cpu_threshold:\n",
        "            scale_containers(container.attrs['Config']['Labels']['com.docker.compose.service'], 2)\n",
        "            print(f\"Scaling up {container.name} due to high CPU usage.\")\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        monitor_containers()\n"
      ],
      "metadata": {
        "id": "5lTvXlWiPUml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Scenario: You have a Docker image stored on a private registry. Develop a script in Bash that authenticates with the registry, pulls the latest version of the image, and runs a container based on that image."
      ],
      "metadata": {
        "id": "gTga7dbzP-KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "\n",
        "# Private registry credentials\n",
        "registry_username=\"your-registry-username\"\n",
        "registry_password=\"your-registry-password\"\n",
        "\n",
        "# Docker registry URL\n",
        "registry_url=\"your-registry-url\"\n",
        "\n",
        "# Image details\n",
        "image_name=\"your-image-name\"\n",
        "image_tag=\"latest\"\n",
        "\n",
        "# Authenticate with the private registry\n",
        "docker login -u \"$registry_username\" -p \"$registry_password\" \"$registry_url\"\n",
        "\n",
        "# Pull the latest version of the image\n",
        "docker pull \"$registry_url/$image_name:$image_tag\"\n",
        "\n",
        "# Run a container based on the pulled image\n",
        "docker run -d \"$registry_url/$image_name:$image_tag\"\n"
      ],
      "metadata": {
        "id": "r82twjvNP5Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TOPIC: Airflow**"
      ],
      "metadata": {
        "id": "lC2cL_RoQLws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Scenario: You have a data pipeline that requires executing a shell command as part of a task. Create an Airflow DAG that includes a BashOperator to execute a specific shell command."
      ],
      "metadata": {
        "id": "_2VzuTyPQN_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "from datetime import datetime\n",
        "\n",
        "default_args = {\n",
        "    'start_date': datetime(2023, 7, 12),\n",
        "    'retries': 3,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "dag = DAG(\n",
        "    'shell_command_dag',\n",
        "    default_args=default_args,\n",
        "    description='DAG for executing a shell command',\n",
        "    schedule_interval='0 0 * * *',  # Run daily at midnight\n",
        ")\n",
        "\n",
        "execute_shell_command = BashOperator(\n",
        "    task_id='execute_shell_command',\n",
        "    bash_command='your_shell_command_here',\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "execute_shell_command\n"
      ],
      "metadata": {
        "id": "zxUIf9I-QHZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Scenario: You want to create dynamic tasks in Airflow based on a list of inputs. Design an Airflow DAG that generates tasks dynamically using PythonOperator, where each task processes an element from the input list."
      ],
      "metadata": {
        "id": "4llgEXRXQdMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime\n",
        "\n",
        "default_args = {\n",
        "    'start_date': datetime(2023, 7, 12),\n",
        "    'retries': 3,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "dag = DAG(\n",
        "    'dynamic_task_dag',\n",
        "    default_args=default_args,\n",
        "    description='DAG for generating dynamic tasks',\n",
        "    schedule_interval=None,  # Set to None to disable automatic scheduling\n",
        ")\n",
        "\n",
        "def process_element(element):\n",
        "    # Add your processing logic here\n",
        "    print(f\"Processing element: {element}\")\n",
        "\n",
        "input_list = ['element1', 'element2', 'element3']  # Replace with your actual input list\n",
        "\n",
        "for element in input_list:\n",
        "    task_id = f\"process_{element}\"\n",
        "    process_task = PythonOperator(\n",
        "        task_id=task_id,\n",
        "        python_callable=process_element,\n",
        "        op_args=[element],\n",
        "        dag=dag,\n",
        "    )\n",
        "\n",
        "    process_task"
      ],
      "metadata": {
        "id": "9VwPKWUoQaxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Scenario: You need to set up a complex task dependency in Airflow, where Task B should start only if Task A has successfully completed. Implement this dependency using the \"TriggerDagRunOperator\" in Airflow."
      ],
      "metadata": {
        "id": "3UpKyL6hQq7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.dagrun_operator import TriggerDagRunOperator\n",
        "from airflow.operators.sensors import ExternalTaskSensor\n",
        "from datetime import datetime\n",
        "\n",
        "default_args = {\n",
        "    'start_date': datetime(2023, 7, 12),\n",
        "    'retries': 3,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "dag = DAG(\n",
        "    'complex_dependency_dag',\n",
        "    default_args=default_args,\n",
        "    description='DAG with complex task dependency',\n",
        "    schedule_interval=None,  # Set to None to disable automatic scheduling\n",
        ")\n",
        "\n",
        "task_a = ExternalTaskSensor(\n",
        "    task_id='task_a',\n",
        "    external_dag_id='your_dag_id',\n",
        "    external_task_id='task_a',\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "task_b = TriggerDagRunOperator(\n",
        "    task_id='task_b',\n",
        "    trigger_dag_id='your_dag_id',\n",
        "    execution_date=\"{{ execution_date }}\",\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "task_a >> task_b\n"
      ],
      "metadata": {
        "id": "OvIgbceuQmzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TOPIC: Sqoop**"
      ],
      "metadata": {
        "id": "R6UrEQ5PQ1oy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Scenario: You want to import data from an Oracle database into Hadoop using Sqoop, but you only need to import specific columns from a specific table. Write a Sqoop command that performs the import, including the necessary arguments for column selection and table mapping."
      ],
      "metadata": {
        "id": "jr7OS7FJQ3sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:oracle:thin:@<database_host>:<port>:<database_name> \\\n",
        "--username <username> \\\n",
        "--password <password> \\\n",
        "--table <table_name> \\\n",
        "--columns \"<column1>,<column2>,<column3>\" \\\n",
        "--target-dir <target_directory> \\\n",
        "--as-textfile \\\n",
        "--m <num_mappers>"
      ],
      "metadata": {
        "id": "wErHS5YqQyCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Scenario: You have a requirement to perform an incremental import of data from a MySQL database into Hadoop using Sqoop. Design a Sqoop command that imports only the new or updated records since the last import."
      ],
      "metadata": {
        "id": "5Uf0Y6HNRKpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sqoop import \\\n",
        "--connect jdbc:mysql://<database_host>:<port>/<database_name> \\\n",
        "--username <username> \\\n",
        "--password <password> \\\n",
        "--table <table_name> \\\n",
        "--target-dir <target_directory> \\\n",
        "--as-textfile \\\n",
        "--incremental append \\\n",
        "--check-column <column_name> \\\n",
        "--last-value <last_imported_value> \\\n",
        "--m <num_mappers>"
      ],
      "metadata": {
        "id": "Q777iJZ3RF9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Scenario: You need to export data from Hadoop to a Microsoft SQL Server database using Sqoop. Develop a Sqoop command that exports the data, considering factors like database connection details, table mapping, and appropriate data types.\n"
      ],
      "metadata": {
        "id": "PCFylvuXRSqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sqoop export \\\n",
        "--connect \"jdbc:sqlserver://<database_host>:<port>;database=<database_name>\" \\\n",
        "--username <username> \\\n",
        "--password <password> \\\n",
        "--table <table_name> \\\n",
        "--export-dir <export_directory> \\\n",
        "--input-fields-terminated-by ',' \\\n",
        "--input-lines-terminated-by '\\n' \\\n",
        "--input-null-string '\\\\N' \\\n",
        "--input-null-non-string '\\\\N' \\\n",
        "--columns \"<column1>,<column2>,<column3>\" \\\n",
        "--batch \\\n",
        "--m <num_mappers>"
      ],
      "metadata": {
        "id": "1VOdNv0LROlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BX--R0lyRWrd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}