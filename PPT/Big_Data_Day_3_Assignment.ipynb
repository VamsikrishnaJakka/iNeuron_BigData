{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop."
      ],
      "metadata": {
        "id": "Rl41t0JdYfg0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq8Rs0sNYNid"
      },
      "outputs": [],
      "source": [
        "import configparser\n",
        "\n",
        "def display_hadoop_components(config_file_path):\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(config_file_path)\n",
        "\n",
        "    if 'core-site' in config:\n",
        "        core_site = config['core-site']\n",
        "        if 'fs.defaultFS' in core_site:\n",
        "            print(\"Hadoop Distributed File System (HDFS) configured at:\", core_site['fs.defaultFS'])\n",
        "\n",
        "    if 'yarn-site' in config:\n",
        "        yarn_site = config['yarn-site']\n",
        "        if 'yarn.resourcemanager.address' in yarn_site:\n",
        "            print(\"Resource Manager configured at:\", yarn_site['yarn.resourcemanager.address'])\n",
        "\n",
        "    if 'mapred-site' in config:\n",
        "        mapred_site = config['mapred-site']\n",
        "        if 'mapreduce.framework.name' in mapred_site:\n",
        "            print(\"MapReduce Framework configured:\", mapred_site['mapreduce.framework.name'])\n",
        "\n",
        "# Example usage\n",
        "config_file = '/path/to/hadoop-config-file.xml'\n",
        "display_hadoop_components(config_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory."
      ],
      "metadata": {
        "id": "_VJzPAs9Yqt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.hdfs as hdfs\n",
        "\n",
        "def calculate_total_file_size(hdfs_host, hdfs_port, directory):\n",
        "    total_size = 0\n",
        "    fs = hdfs.connect(host=hdfs_host, port=hdfs_port)\n",
        "\n",
        "    # List all files in the directory\n",
        "    files = fs.ls(directory, detail=True)\n",
        "\n",
        "    # Iterate through each file and sum up the file sizes\n",
        "    for file in files:\n",
        "        total_size += file['size']\n",
        "\n",
        "    fs.close()\n",
        "\n",
        "    return total_size\n",
        "\n",
        "#Driver usage\n",
        "hdfs_host = 'your_hdfs_host'\n",
        "hdfs_port = your_hdfs_port\n",
        "directory_path = '/your/hdfs/directory'\n",
        "\n",
        "total_size = calculate_total_file_size(hdfs_host, hdfs_port, directory_path)\n",
        "print(\"Total file size:\", total_size)\n"
      ],
      "metadata": {
        "id": "Su3sIS9UYjdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach."
      ],
      "metadata": {
        "id": "gTbQ8KNVCRJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import re\n",
        "\n",
        "WORD_RE = re.compile(r\"\\b\\w+\\b\")  # Regular expression to match words\n",
        "\n",
        "class TopNWords(MRJob):\n",
        "\n",
        "    def mapper_get_words(self, _, line):\n",
        "        words = WORD_RE.findall(line)\n",
        "        for word in words:\n",
        "            yield word.lower(), 1\n",
        "\n",
        "    def combiner_count_words(self, word, counts):\n",
        "        yield word, sum(counts)\n",
        "\n",
        "    def reducer_count_words(self, word, counts):\n",
        "        yield None, (sum(counts), word)\n",
        "\n",
        "    def reducer_find_top_n(self, _, word_count_pairs):\n",
        "        n = 10  # Specify the value of N for top N words\n",
        "        top_n = sorted(word_count_pairs, reverse=True)[:n]\n",
        "        for count, word in top_n:\n",
        "            yield word, count\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_get_words,\n",
        "                   combiner=self.combiner_count_words,\n",
        "                   reducer=self.reducer_count_words),\n",
        "            MRStep(reducer=self.reducer_find_top_n)\n",
        "        ]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    TopNWords.run()\n",
        "\n",
        "\n",
        "#Run the above output in the terminal by saving file as top_most_frequent_elements.py\n",
        "python top_most_frequent_elements.py -r hadoop --hadoop-streaming-jar /path/to/hadoop-streaming.jar input_file.txt\n"
      ],
      "metadata": {
        "id": "z5phflaLCfce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API."
      ],
      "metadata": {
        "id": "lP4g79xVDACS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def check_namenode_health(namenode_host, namenode_port):\n",
        "    url = f\"http://{namenode_host}:{namenode_port}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        live_nodes = data[\"beans\"][0][\"LiveNodes\"]\n",
        "        dead_nodes = data[\"beans\"][0][\"DeadNodes\"]\n",
        "\n",
        "        print(\"NameNode Health Status:\")\n",
        "        print(\"Live Nodes:\", live_nodes)\n",
        "        print(\"Dead Nodes:\", dead_nodes)\n",
        "    else:\n",
        "        print(\"Error accessing NameNode status\")\n",
        "\n",
        "def check_datanode_health(namenode_host, namenode_port):\n",
        "    url = f\"http://{namenode_host}:{namenode_port}/dfshealth.html\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        if \"Datanodes available\" in response.text:\n",
        "            print(\"DataNode Health Status: All DataNodes are available\")\n",
        "        else:\n",
        "            print(\"DataNode Health Status: Some DataNodes are not available\")\n",
        "    else:\n",
        "        print(\"Error accessing DataNode status\")\n",
        "\n",
        "# Example usage\n",
        "nn_host = 'your_namenode_host'\n",
        "nn_port = your_namenode_port\n",
        "\n",
        "check_namenode_health(nn_host, nn_port)\n",
        "check_datanode_health(nn_host, nn_port)\n"
      ],
      "metadata": {
        "id": "31ISTrYYCtCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Develop a Python program that lists all the files and directories in a specific HDFS path."
      ],
      "metadata": {
        "id": "Pp7SIy0EDQSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.hdfs as hdfs\n",
        "\n",
        "def list_hdfs_files(hdfs_host, hdfs_port, hdfs_path):\n",
        "    fs = hdfs.connect(host=hdfs_host, port=hdfs_port)\n",
        "\n",
        "    # List all files and directories in the given HDFS path\n",
        "    files = fs.ls(hdfs_path)\n",
        "\n",
        "    for file in files:\n",
        "        print(file)\n",
        "\n",
        "    fs.close()\n",
        "\n",
        "# Example usage\n",
        "hdfs_host = 'your_hdfs_host'\n",
        "hdfs_port = your_hdfs_port\n",
        "hdfs_directory = '/your/hdfs/directory'\n",
        "\n",
        "list_hdfs_files(hdfs_host, hdfs_port, hdfs_directory)\n"
      ],
      "metadata": {
        "id": "tBuzagxUDP4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities."
      ],
      "metadata": {
        "id": "n4zRkPZhDbmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def analyze_storage_utilization(namenode_host, namenode_port):\n",
        "    url = f\"http://{namenode_host}:{namenode_port}/dfshealth.html\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.text\n",
        "\n",
        "        # Extract the storage utilization information\n",
        "        start_pos = data.find('<table id=\"dfstable\"')\n",
        "        end_pos = data.find('</table>', start_pos)\n",
        "        table_html = data[start_pos:end_pos + len('</table>')]\n",
        "\n",
        "        # Parse the table HTML to extract the relevant data\n",
        "        storage_data = []\n",
        "        rows = table_html.split('</tr>')\n",
        "        for row in rows:\n",
        "            cols = row.split('</td>')\n",
        "            node_name = cols[0].split('>')[-1]\n",
        "            used_storage = cols[1].split('>')[-1]\n",
        "            percent_used = cols[3].split('>')[-1]\n",
        "            storage_data.append((node_name, used_storage, percent_used))\n",
        "\n",
        "        # Sort the storage data by used storage in descending order\n",
        "        sorted_data = sorted(storage_data, key=lambda x: int(x[1]), reverse=True)\n",
        "\n",
        "        # Display the nodes with highest and lowest storage capacities\n",
        "        print(\"Node with Highest Storage Capacity:\")\n",
        "        print(\"Node Name:\", sorted_data[0][0])\n",
        "        print(\"Used Storage:\", sorted_data[0][1])\n",
        "        print(\"Percentage Used:\", sorted_data[0][2])\n",
        "\n",
        "        print(\"\\nNode with Lowest Storage Capacity:\")\n",
        "        print(\"Node Name:\", sorted_data[-1][0])\n",
        "        print(\"Used Storage:\", sorted_data[-1][1])\n",
        "        print(\"Percentage Used:\", sorted_data[-1][2])\n",
        "\n",
        "    else:\n",
        "        print(\"Error accessing HDFS health page\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "nn_host = 'your_namenode_host'\n",
        "nn_port = your_namenode_port\n",
        "\n",
        "analyze_storage_utilization(nn_host, nn_port)\n"
      ],
      "metadata": {
        "id": "iA11efeQDewK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output."
      ],
      "metadata": {
        "id": "6RUwSthZDnd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "# YARN ResourceManager API endpoint\n",
        "rm_api = 'http://<resourcemanager-host>:<resourcemanager-port>/ws/v1/cluster'\n",
        "\n",
        "# Function to submit a Hadoop job\n",
        "def submit_job(jar_file, main_class, input_path, output_path):\n",
        "    url = f'{rm_api}/apps/new-application'\n",
        "    response = requests.post(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        app_id = data['application-id']\n",
        "\n",
        "        # Submit the job using the obtained application ID\n",
        "        url = f'{rm_api}/apps/{app_id}/submit'\n",
        "        headers = {'Content-Type': 'application/json'}\n",
        "        payload = {\n",
        "            \"application-id\": app_id,\n",
        "            \"application-name\": \"Hadoop Job\",\n",
        "            \"am-container-spec\": {\n",
        "                \"commands\": {\n",
        "                    \"command\": f\"yarn jar {jar_file} {main_class} {input_path} {output_path}\"\n",
        "                }\n",
        "            },\n",
        "            \"application-type\": \"MAPREDUCE\"\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, json=payload, headers=headers)\n",
        "        if response.status_code == 202:\n",
        "            print(f\"Job submitted with application ID: {app_id}\")\n",
        "            return app_id\n",
        "        else:\n",
        "            print(\"Failed to submit the job\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"Failed to create a new application\")\n",
        "        return None\n",
        "\n",
        "# Function to monitor job progress\n",
        "def monitor_job(app_id):\n",
        "    url = f'{rm_api}/apps/{app_id}'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        state = data['app']['state']\n",
        "        progress = data['app']['progress']\n",
        "\n",
        "        print(f\"Job state: {state}\")\n",
        "        print(f\"Job progress: {progress}%\")\n",
        "\n",
        "        while state not in ['FINISHED', 'FAILED', 'KILLED']:\n",
        "            time.sleep(5)\n",
        "            response = requests.get(url)\n",
        "            data = response.json()\n",
        "            state = data['app']['state']\n",
        "            progress = data['app']['progress']\n",
        "            print(f\"Job state: {state}\")\n",
        "            print(f\"Job progress: {progress}%\")\n",
        "\n",
        "        print(\"Job completed!\")\n",
        "    else:\n",
        "        print(\"Failed to fetch job information\")\n",
        "\n",
        "# Function to retrieve job output\n",
        "def retrieve_output(app_id, output_path):\n",
        "    url = f'{rm_api}/apps/{app_id}/state'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        container_id = data['containerId']\n",
        "\n",
        "        url = f'{rm_api}/proxy/{container_id}/ws/v1/mapreduce/jobs'\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            job_id = data['jobs']['job'][0]['id']\n",
        "\n",
        "            url = f'{rm_api}/proxy/{container_id}/ws/v1/mapreduce/jobs/{job_id}/jobattempts'\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                attempt_id = data['jobAttempts']['jobAttempt'][0]['id']\n",
        "\n",
        "                url = f'{rm_api}/proxy/{container_id}/ws/v1/mapreduce/jobs/{job_id}/jobattempts/{attempt_id}/counters'\n",
        "                response = requests.get(url)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    counters = data['jobCounters']['counterGroup']\n",
        "\n",
        "                    for counter in counters:\n",
        "                        if counter['counterGroupName'] == 'org.apache.hadoop.mapreduce.FileSystemCounter':\n",
        "                            for item in counter['counter']:\n",
        "                                if item['name'] == 'FILE_BYTES_READ':\n",
        "                                    print(f\"Total bytes read: {item['totalCounterValue']}\")\n",
        "                                elif item['name'] == 'FILE_BYTES_WRITTEN':\n",
        "                                    print(f\"Total bytes written: {item['totalCounterValue']}\")\n",
        "                else:\n",
        "                    print(\"Failed to retrieve job counters\")\n",
        "            else:\n",
        "                print(\"Failed to retrieve job attempt information\")\n",
        "        else:\n",
        "            print(\"Failed to retrieve job information\")\n",
        "    else:\n",
        "        print(\"Failed to fetch container ID\")\n",
        "\n",
        "# Example usage\n",
        "jar_file = '/path/to/hadoop-job.jar'\n",
        "main_class = 'com.example.hadoop.JobMain'\n",
        "input_path = '/input/data'\n",
        "output_path = '/output/result'\n",
        "\n",
        "app_id = submit_job(jar_file, main_class, input_path, output_path)\n",
        "if app_id:\n",
        "    monitor_job(app_id)\n",
        "    retrieve_output(app_id, output_path)\n"
      ],
      "metadata": {
        "id": "WBTF3VJ8DpiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution."
      ],
      "metadata": {
        "id": "lx6Xe6LMFA6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "# YARN ResourceManager API endpoint\n",
        "rm_api = 'http://<resourcemanager-host>:<resourcemanager-port>/ws/v1/cluster'\n",
        "\n",
        "# Function to submit a Hadoop job with resource requirements\n",
        "def submit_job_with_resources(jar_file, main_class, input_path, output_path, num_containers, container_memory, container_vcores):\n",
        "    url = f'{rm_api}/apps/new-application'\n",
        "    response = requests.post(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        app_id = data['application-id']\n",
        "\n",
        "        # Submit the job using the obtained application ID and set resource requirements\n",
        "        url = f'{rm_api}/apps/{app_id}/submit'\n",
        "        headers = {'Content-Type': 'application/json'}\n",
        "        payload = {\n",
        "            \"application-id\": app_id,\n",
        "            \"application-name\": \"Hadoop Job\",\n",
        "            \"am-container-spec\": {\n",
        "                \"commands\": {\n",
        "                    \"command\": f\"yarn jar {jar_file} {main_class} {input_path} {output_path}\"\n",
        "                },\n",
        "                \"resource\": {\n",
        "                    \"memory\": container_memory,\n",
        "                    \"vCores\": container_vcores\n",
        "                },\n",
        "                \"instances\": num_containers\n",
        "            },\n",
        "            \"application-type\": \"MAPREDUCE\"\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, json=payload, headers=headers)\n",
        "        if response.status_code == 202:\n",
        "            print(f\"Job submitted with application ID: {app_id}\")\n",
        "            return app_id\n",
        "        else:\n",
        "            print(\"Failed to submit the job\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"Failed to create a new application\")\n",
        "        return None\n",
        "\n",
        "# Function to monitor job resource usage\n",
        "def monitor_resource_usage(app_id):\n",
        "    url = f'{rm_api}/apps/{app_id}'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        state = data['app']['state']\n",
        "        progress = data['app']['progress']\n",
        "\n",
        "        print(f\"Job state: {state}\")\n",
        "        print(f\"Job progress: {progress}%\")\n",
        "\n",
        "        while state not in ['FINISHED', 'FAILED', 'KILLED']:\n",
        "            url = f'{rm_api}/apps/{app_id}/appattempts'\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                attempts = data['appAttempts']['appAttempt']\n",
        "                latest_attempt = attempts[-1]\n",
        "                container_id = latest_attempt['containerId']\n",
        "\n",
        "                url = f'{rm_api}/nodes/{container_id}/containers'\n",
        "                response = requests.get(url)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    containers = data['containers']['container']\n",
        "\n",
        "                    for container in containers:\n",
        "                        print(f\"Container ID: {container['id']}\")\n",
        "                        print(f\"Allocated Memory: {container['allocatedMB']} MB\")\n",
        "                        print(f\"Allocated vCores: {container['allocatedVCores']}\")\n",
        "                        print(f\"Used Memory: {container['usedMemoryMB']} MB\")\n",
        "                        print(f\"Used vCores: {container['usedVirtualCores']}\")\n",
        "                        print(\"-----------------------\")\n",
        "                else:\n",
        "                    print(\"Failed to fetch container information\")\n",
        "            else:\n",
        "                print(\"Failed to fetch job attempt information\")\n",
        "\n",
        "            time.sleep(5)\n",
        "            response = requests.get(url)\n",
        "            data = response.json()\n",
        "            state = data['app']['state']\n",
        "            progress = data['app']['progress']\n",
        "            print(f\"Job state: {state}\")\n",
        "            print(f\"Job progress: {progress}%\")\n",
        "\n",
        "        print(\"Job completed!\")\n",
        "    else:\n",
        "        print(\"Failed to fetch job information\")\n",
        "\n",
        "# Example usage\n",
        "jar_file = '/path/to/hadoop-job.jar'\n",
        "main_class = 'com.example.hadoop.JobMain'\n",
        "input_path = '/input/data'\n",
        "output_path = '/output/result'\n",
        "num_containers = 5\n",
        "container_memory = 1024\n",
        "container_vcores = 2\n",
        "\n",
        "app_id = submit_job_with_resources(jar_file, main_class, input_path, output_path, num_containers, container_memory, container_vcores)\n",
        "if app_id:\n",
        "    monitor_resource_usage(app_id)\n"
      ],
      "metadata": {
        "id": "dSDjNmEkFC1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
      ],
      "metadata": {
        "id": "JvLMq_ACFVu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mrjob.job import MRJob\n",
        "import time\n",
        "\n",
        "# MapReduce job to calculate word count\n",
        "class WordCountJob(MRJob):\n",
        "\n",
        "    def configure_args(self):\n",
        "        super(WordCountJob, self).configure_args()\n",
        "        self.add_passthru_arg('--split-size', type=int, default=64,\n",
        "                              help='Input split size in megabytes')\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        words = line.strip().split()\n",
        "        for word in words:\n",
        "            yield word.lower(), 1\n",
        "\n",
        "    def combiner(self, word, counts):\n",
        "        yield word, sum(counts)\n",
        "\n",
        "    def reducer(self, word, counts):\n",
        "        yield word, sum(counts)\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            self.mr(self.mapper, self.combiner, self.reducer)\n",
        "        ]\n",
        "\n",
        "\n",
        "# Function to run the WordCountJob with different input split sizes\n",
        "def compare_input_split_sizes(input_file, split_sizes):\n",
        "    print(\"Comparing input split sizes:\")\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "    for split_size in split_sizes:\n",
        "        job = WordCountJob(args=[input_file, f'--split-size={split_size}'])\n",
        "\n",
        "        start_time = time.time()\n",
        "        with job.make_runner() as runner:\n",
        "            runner.run()\n",
        "            elapsed_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Split Size: {split_size} MB\")\n",
        "        print(f\"Job Execution Time: {elapsed_time:.2f} seconds\")\n",
        "        print(\"-----------------------------\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_file = '/path/to/input.txt'\n",
        "split_sizes = [64, 128, 256]  # Specify different input split sizes in megabytes\n",
        "\n",
        "compare_input_split_sizes(input_file, split_sizes)\n"
      ],
      "metadata": {
        "id": "bJEPBvxsFYAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7Uga2AdFmxC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}