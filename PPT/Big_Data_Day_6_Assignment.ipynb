{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Working with RDDs:\n",
        "   a) Write a Python program to create an RDD from a local data source.\n",
        "   b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
        "   c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n"
      ],
      "metadata": {
        "id": "ZXI8vcvEKsj-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iptjBlO1Kc0E"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDDExample\")\n",
        "\n",
        "# Create an RDD from a local data source (list)\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Perform transformations and actions on the RDD\n",
        "squared_rdd = rdd.map(lambda x: x ** 2)  # Apply a transformation: square each element\n",
        "filtered_rdd = squared_rdd.filter(lambda x: x > 10)  # Apply a transformation: filter elements > 10\n",
        "sum_result = filtered_rdd.reduce(lambda x, y: x + y)  # Apply an action: calculate sum\n",
        "\n",
        "# Analyze and manipulate data using RDD operations\n",
        "count = rdd.count()  # Count the number of elements in the RDD\n",
        "min_value = rdd.min()  # Find the minimum value in the RDD\n",
        "max_value = rdd.max()  # Find the maximum value in the RDD\n",
        "\n",
        "# Print the results\n",
        "print(\"Original RDD: {}\".format(rdd.collect()))\n",
        "print(\"Squared RDD: {}\".format(squared_rdd.collect()))\n",
        "print(\"Filtered RDD: {}\".format(filtered_rdd.collect()))\n",
        "print(\"Sum of Filtered RDD: {}\".format(sum_result))\n",
        "print(\"Number of elements in RDD: {}\".format(count))\n",
        "print(\"Minimum value in RDD: {}\".format(min_value))\n",
        "print(\"Maximum value in RDD: {}\".format(max_value))\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Spark DataFrame Operations:\n",
        "   a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
        "   b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
        "   c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n"
      ],
      "metadata": {
        "id": "ndbsMGPBLQk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
        "\n",
        "# Load a CSV file into a DataFrame\n",
        "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Perform common DataFrame operations\n",
        "filtered_df = df.filter(df[\"age\"] > 25)  # Apply a filter: select rows where age > 25\n",
        "grouped_df = df.groupBy(\"gender\").count()  # Group by gender and count occurrences\n",
        "joined_df = df.join(grouped_df, \"gender\")  # Join with grouped_df based on the \"gender\" column\n",
        "\n",
        "# Apply Spark SQL queries on the DataFrame\n",
        "df.createOrReplaceTempView(\"people\")  # Create a temporary view for the DataFrame\n",
        "sql_result = spark.sql(\"SELECT name, age FROM people WHERE age > 30\")  # Execute a SQL query\n",
        "\n",
        "# Show the results\n",
        "df.show()\n",
        "filtered_df.show()\n",
        "grouped_df.show()\n",
        "joined_df.show()\n",
        "sql_result.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "aY-QNrwYLNmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Spark Streaming:\n",
        "  a) Write a Python program to create a Spark Streaming application.\n",
        "   b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
        "   c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n"
      ],
      "metadata": {
        "id": "SIcYcY8ELiA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a Spark Streaming Context with a batch interval of 1 second\n",
        "ssc = StreamingContext(sparkContext, 1)\n",
        "\n",
        "# Configure the streaming application to consume data from Kafka\n",
        "kafka_params = {\n",
        "    \"bootstrap.servers\": \"localhost:9092\",\n",
        "    \"group.id\": \"my_consumer_group\",\n",
        "    \"auto.offset.reset\": \"latest\"\n",
        "}\n",
        "topic = \"my_topic\"\n",
        "stream = KafkaUtils.createDirectStream(ssc, [topic], kafka_params)\n",
        "\n",
        "# Implement streaming transformations and actions\n",
        "lines = stream.map(lambda x: x[1])  # Extract the value from the Kafka message\n",
        "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
        "                   .map(lambda word: (word, 1)) \\\n",
        "                   .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the word counts\n",
        "word_counts.pprint()\n",
        "\n",
        "# Start the streaming context\n",
        "ssc.start()\n",
        "\n",
        "# Await termination or stop after a specified duration\n",
        "ssc.awaitTerminationOrTimeout(30)  # Stop the streaming context after 30 seconds\n",
        "\n",
        "# Stop the streaming context\n",
        "ssc.stop(stopSparkContext=True, stopGraceFully=True)\n"
      ],
      "metadata": {
        "id": "-GbcXy_kLekw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Spark SQL and Data Source Integration:\n",
        "   a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
        "   b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
        "   c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.\n"
      ],
      "metadata": {
        "id": "DiG2WR2nLzuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQLExample\").getOrCreate()\n",
        "\n",
        "# Connect Spark with a relational database (e.g., MySQL, PostgreSQL)\n",
        "db_url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
        "db_properties = {\n",
        "    \"user\": \"username\",\n",
        "    \"password\": \"password\",\n",
        "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
        "}\n",
        "table_name = \"mytable\"\n",
        "\n",
        "# Read data from the database table into a DataFrame\n",
        "df = spark.read.jdbc(url=db_url, table=table_name, properties=db_properties)\n",
        "\n",
        "# Perform SQL operations on the data stored in the database using Spark SQL\n",
        "df.createOrReplaceTempView(\"mydata\")  # Create a temporary view for the DataFrame\n",
        "sql_result = spark.sql(\"SELECT * FROM mydata WHERE age > 30\")  # Execute a SQL query\n",
        "\n",
        "# Explore integration capabilities with other data sources (e.g., HDFS or Amazon S3)\n",
        "hdfs_path = \"hdfs://localhost:9000/path/to/hdfs/data.parquet\"\n",
        "s3_path = \"s3a://bucket/path/to/s3/data.parquet\"\n",
        "\n",
        "# Read data from HDFS into a DataFrame\n",
        "hdfs_df = spark.read.parquet(hdfs_path)\n",
        "\n",
        "# Read data from Amazon S3 into a DataFrame\n",
        "s3_df = spark.read.parquet(s3_path)\n",
        "\n",
        "# Perform operations on the data from different sources\n",
        "combined_df = df.union(hdfs_df).union(s3_df)  # Combine data from different sources\n",
        "\n",
        "# Show the results\n",
        "df.show()\n",
        "sql_result.show()\n",
        "combined_df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "t0dqnCkNLpaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A_x6d-t9L0yo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}